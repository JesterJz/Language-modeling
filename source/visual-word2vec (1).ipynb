{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.models as w2v\n",
    "import sklearn.decomposition as dcmp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import re\n",
    "import nltk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "<h1>Visualize Word2Vec</h1>\n",
    "<p>Word2vec + PCA + Clustering</p>\n",
    "\"\"\"\n",
    "<div>__author__ = \"Aubry Cholleton\"</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_w2v = '../Data/W2VModelVN.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc45221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.models as w2v\n",
    "import sklearn.decomposition as dcmp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "Visualize Word2Vec\n",
    "Word2vec + PCA + Clustering\n",
    "\"\"\"\n",
    "__author__ = \"Aubry Cholleton\"\n",
    "\n",
    "path_model_w2v = '../Data/W2VModelVN.bin'\n",
    "\n",
    "\n",
    "class SemanticMap:\n",
    "    def __init__(self, model_path):\n",
    "        print ('Loading model ...')\n",
    "        self.model = gensim.models.KeyedVectors.load_word2vec_format(model_path, fvocab=None, binary=True, encoding='utf8')\n",
    "        print ('Ready')\n",
    "        #mô tả vecto 300 chiều của từ hoài\n",
    "        print(self.model['hoài'])\n",
    "\n",
    "    def __split_words(self, input_string):\n",
    "        # return re.findall(r\"[\\w']+\", input_string)\n",
    "        return (input_string,)\n",
    "\n",
    "    def __clean_words(self, words):\n",
    "        clean_words = []\n",
    "        for w in words:\n",
    "            clean_words.append(w)\n",
    "        return clean_words\n",
    "\n",
    "    def __remove_stop_words(self, words):\n",
    "        # return [w for w in words if not w in nltk.corpus.stopwords.words('english')]\n",
    "        return words\n",
    "\n",
    "    def __get_non_compositional_entity_vector(self, entity):\n",
    "        print ('get_non_compositional_entity_vector: ', entity)\n",
    "        word = entity[0]\n",
    "        print ('word: ', word)\n",
    "        ver_word = self.model[word]\n",
    "        return ver_word\n",
    "\n",
    "    def __get_compositional_entity_vector(self, entity):\n",
    "        array = np.array(self.model[entity[0]])\n",
    "        for ind in range (1, len(entity)):\n",
    "            array = array + np.array(self.model[entity[ind]])\n",
    "        return array/len(entity)\n",
    "\n",
    "    def __get_vector(self, term):\n",
    "        words = self.__remove_stop_words(self.__clean_words(self.__split_words(term)))\n",
    "\n",
    "        if len(words) < 1:\n",
    "            print ('All the terms have been filtered.')\n",
    "            raise\n",
    "        if len(words) == 1:\n",
    "            try:\n",
    "                return self.__get_non_compositional_entity_vector(words)\n",
    "            except:\n",
    "                print ('Out-of-vocabulary entity')\n",
    "                raise\n",
    "        elif len(words) < 4:\n",
    "            try:\n",
    "                return self.__get_compositional_entity_vector(words)\n",
    "            except:\n",
    "                print ('Out-of-vocabulary word in compositional entity')\n",
    "                raise\n",
    "        else:\n",
    "            print ('Entity is too long.')\n",
    "            raise\n",
    "\n",
    "    def __reduce_dimensionality(self, word_vectors, dimension=2):\n",
    "        data = np.array(word_vectors)\n",
    "        pca = dcmp.PCA(n_components=dimension)\n",
    "        pca.fit(data)\n",
    "        return pca.transform(data)\n",
    "\n",
    "    def cluster_results(self, data, threshold=0.13):\n",
    "        return hcluster.fclusterdata(data, threshold, criterion=\"distance\")\n",
    "\n",
    "    def map_words(self, words, sizes):\n",
    "        final_words = []\n",
    "        final_sizes = []\n",
    "        vectors = []\n",
    "\n",
    "        for word in words:\n",
    "            try:\n",
    "                vect = self.__get_vector(word)\n",
    "                vectors.append(vect)\n",
    "                if sizes is not None:\n",
    "                    final_sizes.append(sizes[words.index(word)])\n",
    "                final_words.append(word)\n",
    "            except Exception:\n",
    "                print ('not valid ' + word)\n",
    "\n",
    "        return vectors, final_words, final_sizes\n",
    "\n",
    "    def plot(self, vectors, lemmas, clusters, sizes=80):\n",
    "        if sizes == []:\n",
    "            sizes = 80\n",
    "        plt.scatter(vectors[:, 0], vectors[:, 1], s=sizes, c=clusters)\n",
    "        for label, x, y in zip(lemmas, vectors[:, 0], vectors[:, 1]):\n",
    "            plt.annotate(\n",
    "                label,\n",
    "                xy = (x, y), xytext = (-20, 20),\n",
    "                textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "                bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
    "                arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def map_cluster_plot(self, words, sizes, threshold):\n",
    "        vectors, words, sizes = self.map_words(words, sizes)\n",
    "        vectors = self.__reduce_dimensionality(vectors)\n",
    "        clusters = self.cluster_results(vectors, threshold)\n",
    "        self.plot(vectors, words, clusters, sizes)\n",
    "\n",
    "    def print_results(self, words, clusters):\n",
    "        print( \"words\")\n",
    "        print (clusters.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbf in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c9a9793e992b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mmapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSemanticMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_model_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcli\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-f15cc502b4e5>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Loading model ...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Ready'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#mô tả vecto 300 chiều của từ hoài\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \"\"\"\n\u001b[0;32m   1546\u001b[0m         \u001b[1;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1547\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             _word2vec_read_binary(fin, result, counts,\n\u001b[0m\u001b[0;32m    286\u001b[0m                 vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)\n\u001b[0;32m    287\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_word2vec_read_binary\u001b[1;34m(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mnew_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_chunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_chunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         processed_words, chunk = _add_bytes_to_result(\n\u001b[0m\u001b[0;32m    205\u001b[0m             result, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)\n\u001b[0;32m    206\u001b[0m         \u001b[0mtot_processed_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprocessed_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_add_bytes_to_result\u001b[1;34m(result, counts, chunk, vocab_size, vector_size, datatype, unicode_errors)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi_space\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[1;31m# Some binary files are reported to have obsolete new line in the beginning of word, remove it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbf in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "def cli(mapper_cli):\n",
    "    # while True:\n",
    "        encoding = 'utf-8' if sys.stdin.encoding in (None, 'ascii') else sys.stdin.encoding\n",
    "        line = input('Enter words or MWEs > ')\n",
    "        # if line == 'exit':\n",
    "            # break\n",
    "        mapper_cli.map_cluster_plot(line.split(','), None, 0.2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mapper = SemanticMap(path_model_w2v)\n",
    "    cli(mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7938e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nhap dòng nào vào\n",
    "\n",
    "#sông,hồ,núi,biển,trâu,bò,gà,chó,mèo,heo,vịt,Mặt_Trời,Trái_Đất,vì_sao,con_cái,bố,mẹ,vợ,chồng,anh,em"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cdf929322205b9e43d98d7c4cf91987317ab1d4d13086b9febd1487a1a96261"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
